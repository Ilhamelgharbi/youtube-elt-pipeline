# üìã YouTube ELT Pipeline - Guide √âtape par √âtape

## üéØ Objectif du Projet
Cr√©er un pipeline ELT automatis√© pour extraire, charger et transformer les donn√©es YouTube avec Apache Airflow, PostgreSQL et Soda Core.

---

## üìÖ Timeline du Projet (10 jours)
- **Jours 1-2**: Setup et DAG d'extraction (produce_JSON)
- **Jours 3-4**: PostgreSQL et DAG de chargement (update_db)
- **Jours 5-6**: Data Quality avec Soda Core
- **Jours 7-8**: Tests et CI/CD
- **Jours 9-10**: Documentation et finalisation

---

## üöÄ Phase 1: Initialisation du Projet (Jour 1)

### ‚úÖ √âtape 1.1: Installer Astro CLI
```powershell
# V√©rifier si Astro CLI est install√©
astro version

# Si non install√©, installer avec:
winget install -e --id Astronomer.Astro
```

### ‚úÖ √âtape 1.2: Initialiser le Projet Astro
```powershell
# Cr√©er le dossier du projet
cd C:\Users\user\Desktop\version-final

# Initialiser Astro (d√©j√† fait ‚úÖ)
astro dev init

# Structure cr√©√©e:
# ‚îú‚îÄ‚îÄ dags/              # Vos DAGs Airflow
# ‚îú‚îÄ‚îÄ include/           # Donn√©es et scripts
# ‚îú‚îÄ‚îÄ plugins/           # Plugins Airflow personnalis√©s
# ‚îú‚îÄ‚îÄ tests/             # Tests unitaires
# ‚îú‚îÄ‚îÄ Dockerfile         # Configuration Docker
# ‚îú‚îÄ‚îÄ requirements.txt   # D√©pendances Python
# ‚îú‚îÄ‚îÄ .env              # Variables d'environnement
# ‚îî‚îÄ‚îÄ packages.txt       # Packages syst√®me
```

### ‚úÖ √âtape 1.3: Configurer les D√©pendances
```powershell
# √âditer requirements.txt (d√©j√† fait ‚úÖ)
```

**Contenu de requirements.txt:**
```txt
# YouTube API
google-api-python-client==2.108.0
google-auth==2.23.4
isodate==0.6.1

# PostgreSQL
psycopg2-binary==2.9.7
apache-airflow-providers-postgres==5.7.1

# Data Quality
soda-core-postgres==3.1.6

# Data Processing
pandas==2.1.3
numpy==1.24.4

# Testing
pytest==7.4.3
```

### ‚úÖ √âtape 1.4: Configurer les Variables d'Environnement
```powershell
# √âditer .env (d√©j√† fait ‚úÖ)
```

**Contenu de .env:**
```env
# YouTube API
YOUTUBE_API_KEY=AIzaSyDyhE7sl0qtEhsZX0e8WXSdYNR6bEikHXQ
YOUTUBE_CHANNEL_ID=UCX6OQ3DkcsbYNE6H8uQQuVA
YOUTUBE_CHANNEL_HANDLE=MrBeast
YOUTUBE_MAX_RESULTS=50

# PostgreSQL Data Warehouse
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=youtube_dwh
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

---

## üìä Phase 2: DAG d'Extraction - produce_JSON (Jours 1-2)

### ‚úÖ √âtape 2.1: Cr√©er le DAG d'Extraction
**Fichier: `dags/youtube_extract.py`** (d√©j√† cr√©√© ‚úÖ)

Ce DAG fait:
1. Se connecte √† l'API YouTube v3
2. R√©cup√®re les vid√©os de la cha√Æne MrBeast
3. Extrait les m√©tadonn√©es (titre, vues, likes, dur√©e, etc.)
4. Sauvegarde dans un fichier JSON horodat√©
5. Stocke le chemin dans XCom pour les DAGs suivants

### ‚úÖ √âtape 2.2: Cr√©er les Utilitaires YouTube
**Fichier: `dags/utils/youtube_utils.py`** (d√©j√† cr√©√© ‚úÖ)

Contient:
- `YouTubeAPIClient`: Classe pour g√©rer l'API YouTube
- `iso_duration_to_readable()`: Convertir dur√©e ISO 8601
- `clean_video_data()`: Nettoyer les donn√©es brutes
- `validate_video_data()`: Valider les donn√©es

### ‚úÖ √âtape 2.3: D√©marrer Airflow Localement
```powershell
# D√©marrer Airflow avec Astro
astro dev start

# Attendre que tous les conteneurs d√©marrent
# Airflow UI: http://localhost:8080
# Username: admin
# Password: admin
```

### ‚úÖ √âtape 2.4: Tester le DAG d'Extraction
```powershell
# 1. Aller sur http://localhost:8080
# 2. Trouver le DAG "produce_JSON"
# 3. Activer le DAG (toggle ON)
# 4. Cliquer sur "Trigger DAG" (‚ñ∂Ô∏è)
# 5. V√©rifier les logs

# Ou via CLI:
astro dev bash
airflow dags test produce_JSON 2025-10-02
exit
```

### ‚úÖ √âtape 2.5: V√©rifier les Fichiers JSON Cr√©√©s
```powershell
# V√©rifier dans le dossier include/youtube_data
ls include/youtube_data/

# Lire un fichier JSON
cat include/youtube_data/MrBeast_20251002_143000.json
```

**Format JSON attendu:**
```json
{
    "channel_handle": "MrBeast",
    "channel_id": "UCX6OQ3DkcsbYNE6H8uQQuVA",
    "extraction_date": "2025-10-02T14:30:00.123456",
    "total_videos": 50,
    "videos": [
        {
            "video_id": "4l97aNza_Zc",
            "title": "Survive 30 Days Chained To Your Ex",
            "published_at": "2025-09-13T16:00:01Z",
            "duration_iso": "PT37M4S",
            "duration_readable": "37:04",
            "duration_seconds": 2224,
            "view_count": 54506132,
            "like_count": 1833636,
            "comment_count": 27466,
            "thumbnail_url": "https://..."
        }
    ]
}
```

---

## üóÑÔ∏è Phase 3: PostgreSQL Data Warehouse (Jours 3-4)

### ‚úÖ √âtape 3.1: Cr√©er les Scripts SQL
**Fichier: `include/sql/create_schemas.sql`**

```sql
-- Cr√©er les sch√©mas
CREATE SCHEMA IF NOT EXISTS staging;
CREATE SCHEMA IF NOT EXISTS core;

-- Sch√©ma STAGING: Donn√©es brutes
CREATE TABLE IF NOT EXISTS staging.youtube_videos_raw (
    id SERIAL PRIMARY KEY,
    video_id VARCHAR(50),
    title TEXT,
    description TEXT,
    published_at TIMESTAMP,
    duration_iso VARCHAR(50),
    duration_seconds INTEGER,
    view_count BIGINT,
    like_count BIGINT,
    comment_count BIGINT,
    channel_id VARCHAR(50),
    channel_handle VARCHAR(100),
    extraction_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Sch√©ma CORE: Donn√©es transform√©es
CREATE TABLE IF NOT EXISTS core.videos (
    video_id VARCHAR(50) PRIMARY KEY,
    title TEXT NOT NULL,
    description TEXT,
    published_at TIMESTAMP NOT NULL,
    duration_seconds INTEGER,
    duration_readable VARCHAR(20),
    channel_id VARCHAR(50) NOT NULL,
    channel_handle VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS core.video_statistics (
    id SERIAL PRIMARY KEY,
    video_id VARCHAR(50) REFERENCES core.videos(video_id),
    view_count BIGINT DEFAULT 0,
    like_count BIGINT DEFAULT 0,
    comment_count BIGINT DEFAULT 0,
    recorded_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Index pour performance
CREATE INDEX IF NOT EXISTS idx_videos_channel ON core.videos(channel_id);
CREATE INDEX IF NOT EXISTS idx_stats_video ON core.video_statistics(video_id);
CREATE INDEX IF NOT EXISTS idx_stats_recorded ON core.video_statistics(recorded_at);
```

### ‚úÖ √âtape 3.2: Cr√©er le DAG update_db
**Fichier: `dags/youtube_load_db.py`**

Ce DAG fait:
1. Lit le dernier fichier JSON cr√©√©
2. Charge les donn√©es dans `staging.youtube_videos_raw`
3. Transforme et nettoie les donn√©es
4. Ins√®re dans `core.videos` et `core.video_statistics`
5. G√®re les doublons et l'historique

### ‚úÖ √âtape 3.3: Configurer la Connexion PostgreSQL
```powershell
# Via l'interface Airflow:
# 1. Aller sur http://localhost:8080
# 2. Admin ‚Üí Connections
# 3. Ajouter une nouvelle connexion:
#    - Conn Id: postgres_dwh
#    - Conn Type: Postgres
#    - Host: postgres
#    - Schema: youtube_dwh
#    - Login: postgres
#    - Password: postgres
#    - Port: 5432

# Ou via CLI:
astro dev bash
airflow connections add postgres_dwh \
    --conn-type postgres \
    --conn-host postgres \
    --conn-schema youtube_dwh \
    --conn-login postgres \
    --conn-password postgres \
    --conn-port 5432
exit
```

### ‚úÖ √âtape 3.4: Tester le DAG update_db
```powershell
# Trigger le DAG
# 1. S'assurer que produce_JSON a √©t√© ex√©cut√©
# 2. Activer update_db
# 3. Trigger le DAG
# 4. V√©rifier les logs
```

### ‚úÖ √âtape 3.5: V√©rifier les Donn√©es dans PostgreSQL
```powershell
# Se connecter √† PostgreSQL
astro dev bash
psql -h postgres -U postgres -d youtube_dwh

# V√©rifier les donn√©es
SELECT COUNT(*) FROM staging.youtube_videos_raw;
SELECT COUNT(*) FROM core.videos;
SELECT COUNT(*) FROM core.video_statistics;

# Voir les derni√®res vid√©os
SELECT video_id, title, view_count, published_at 
FROM core.videos 
ORDER BY published_at DESC 
LIMIT 5;

\q
exit
```

---

## ‚úÖ Phase 4: Data Quality avec Soda Core (Jours 5-6)

### ‚úÖ √âtape 4.1: Cr√©er la Configuration Soda
**Fichier: `include/soda/configuration.yml`**

```yaml
data_source postgres_dwh:
  type: postgres
  host: ${POSTGRES_HOST}
  port: ${POSTGRES_PORT}
  username: ${POSTGRES_USER}
  password: ${POSTGRES_PASSWORD}
  database: ${POSTGRES_DB}
  schema: core
```

### ‚úÖ √âtape 4.2: Cr√©er les Checks de Qualit√©
**Fichier: `include/soda/checks/videos_quality.yml`**

```yaml
checks for videos:
  # Compl√©tude
  - row_count > 0
  - missing_count(video_id) = 0
  - missing_count(title) = 0
  - missing_count(published_at) = 0
  
  # Validit√©
  - invalid_count(video_id) = 0:
      valid length: 11
  - invalid_count(view_count) = 0:
      valid min: 0
  - invalid_count(duration_seconds) = 0:
      valid min: 0
  
  # Unicit√©
  - duplicate_count(video_id) = 0
  
  # Fra√Æcheur
  - freshness(published_at) < 1d

checks for video_statistics:
  - row_count > 0
  - missing_count(video_id) = 0
  - invalid_count(view_count) = 0:
      valid min: 0
```

### ‚úÖ √âtape 4.3: Cr√©er le DAG data_quality
**Fichier: `dags/youtube_data_quality.py`**

Ce DAG:
1. Ex√©cute les checks Soda Core
2. Valide la qualit√© des donn√©es
3. Envoie des alertes si probl√®mes
4. G√©n√®re un rapport de qualit√©

### ‚úÖ √âtape 4.4: Tester les Checks de Qualit√©
```powershell
# Trigger le DAG data_quality
# V√©rifier les logs pour voir les r√©sultats
```

---

## üîó Phase 5: Orchestration des DAGs (Jour 6) ‚úÖ COMPLETE

### ‚úÖ √âtape 5.1: Orchestration via TriggerDagRunOperator (DONE!)

**Votre impl√©mentation actuelle (d√©j√† en place):**

```
produce_JSON (manuel)
     ‚Üì
update_db (manuel)
  ‚îú‚îÄ find_json_task
  ‚îú‚îÄ load_staging_task
  ‚îú‚îÄ transform_core_task
  ‚îú‚îÄ verify_task
  ‚îî‚îÄ trigger_quality_check ‚îÄ‚îÄ‚Üí Automatically triggers data_quality!
```

**Fichier: `dags/youtube_load_db.py` - Lines 372-378**
```python
# Task 5: Trigger data quality validation
trigger_quality_check = TriggerDagRunOperator(
    task_id="trigger_data_quality",
    trigger_dag_id="data_quality",  # ‚úÖ Auto-triggers quality checks!
    wait_for_completion=False,
)

# Dependency chain
find_json_task >> load_staging_task >> transform_core_task >> verify_task >> trigger_quality_check
```

### ‚úÖ √âtape 5.2: Comment Utiliser l'Orchestration

**Option 1: Pipeline Complet (Recommand√©)**
```powershell
# 1. Extraire les donn√©es YouTube
# Airflow UI ‚Üí produce_JSON ‚Üí Trigger DAG

# 2. Charger dans PostgreSQL (d√©clenche automatiquement data_quality!)
# Airflow UI ‚Üí update_db ‚Üí Trigger DAG
#   ‚Ü≥ Cela ex√©cute automatiquement data_quality √† la fin!
```

**Option 2: Via CLI**
```powershell
astro dev bash

# Extraire
airflow dags test produce_JSON 2025-10-02

# Charger (trigger data_quality automatiquement)
airflow dags test update_db 2025-10-02

exit
```

### üí° Pourquoi Cette Approche?

**Avantages:**
- ‚úÖ **Flexibilit√©**: Ex√©cuter `produce_JSON` quotidiennement, `update_db` √† la demande
- ‚úÖ **Debugging**: Chaque DAG a ses propres logs ind√©pendants
- ‚úÖ **Best Practice**: Airflow recommande le d√©couplage des DAGs
- ‚úÖ **R√©utilisabilit√©**: Chaque DAG peut √™tre test√©/ex√©cut√© s√©par√©ment

**Alternative (Non impl√©ment√©e - pas n√©cessaire):**
Si vous voulez un DAG master qui orchestre tout en une seule fois, vous pourriez cr√©er `youtube_pipeline_main.py`, mais ce n'est **pas requis** car vous avez d√©j√† l'orchestration via `TriggerDagRunOperator`

---

## üß™ Phase 6: Tests (Jours 7-8)

### ‚úÖ √âtape 6.1: Cr√©er les Tests Unitaires
**Fichier: `tests/dags/test_youtube_extract.py`**

Tests √† cr√©er:
- Test de la connexion API YouTube
- Test de la conversion de dur√©e
- Test de la validation des donn√©es
- Test de la cr√©ation de fichier JSON

### ‚úÖ √âtape 6.2: Cr√©er les Tests d'Int√©gration
**Fichier: `tests/integration/test_pipeline.py`**

Tests √† cr√©er:
- Test du pipeline complet
- Test de la connexion PostgreSQL
- Test des transformations

### ‚úÖ √âtape 6.3: Ex√©cuter les Tests
```powershell
# Ex√©cuter tous les tests
astro dev pytest

# Ou sp√©cifiquement:
astro dev bash
pytest tests/ -v
exit
```

---

## üöÄ Phase 7: CI/CD avec GitHub Actions (Jour 8)

### ‚úÖ √âtape 7.1: Cr√©er le Workflow GitHub
**Fichier: `.github/workflows/ci-cd.yml`**

Pipeline CI/CD:
1. Lint du code (Black, Flake8)
2. Tests unitaires
3. Tests d'int√©gration
4. Build Docker
5. D√©ploiement (optionnel)

### ‚úÖ √âtape 7.2: Configurer les Secrets GitHub
```
DOCKER_USERNAME
DOCKER_PASSWORD
ASTRO_API_KEY (si d√©ploiement Astronomer)
```

---

## üìä Phase 8: Dashboard (Bonus - Jour 9)

### ‚úÖ √âtape 8.1: Cr√©er le Dashboard Streamlit
**Fichier: `dashboard/app.py`**

Dashboard affichant:
- Nombre total de vid√©os
- Vues totales
- Vid√©os les plus populaires
- √âvolution des statistiques
- Graphiques interactifs

### ‚úÖ √âtape 8.2: Lancer le Dashboard
```powershell
streamlit run dashboard/app.py
```

---

## üìù Phase 9: Documentation (Jour 10)

### ‚úÖ √âtape 9.1: Cr√©er le README Complet
**Fichier: `README.md`**

Sections:
- Description du projet
- Architecture
- Installation
- Configuration
- Utilisation
- Tests
- D√©ploiement
- Screenshots

### ‚úÖ √âtape 9.2: Ajouter la Documentation des DAGs
Documenter chaque DAG avec:
- Description
- Param√®tres
- D√©pendances
- Schedule
- Exemples

---

## üéØ Checklist Finale

### ‚úÖ Fonctionnalit√©s
- [ ] DAG produce_JSON fonctionne
- [ ] DAG update_db charge les donn√©es
- [ ] DAG data_quality valide les donn√©es
- [ ] Pipeline orchestr√© fonctionne bout en bout
- [ ] Gestion des erreurs et retry
- [ ] Logs d√©taill√©s

### ‚úÖ Architecture
- [ ] Structure modulaire
- [ ] Code bien organis√©
- [ ] Configuration centralis√©e
- [ ] Secrets s√©curis√©s

### ‚úÖ PostgreSQL
- [ ] Sch√©mas staging et core cr√©√©s
- [ ] Tables avec index
- [ ] Transformations impl√©ment√©es
- [ ] Acc√®s depuis Airflow et outils externes

### ‚úÖ Tests
- [ ] 20+ tests unitaires
- [ ] Tests d'int√©gration
- [ ] Tests de qualit√© des donn√©es
- [ ] Tous les tests passent

### ‚úÖ Documentation
- [ ] README complet
- [ ] Docstrings sur toutes les fonctions
- [ ] Commentaires dans le code
- [ ] Guide d'installation

### ‚úÖ D√©ploiement
- [ ] Docker fonctionne
- [ ] CI/CD configur√©
- [ ] Screenshots captur√©s
- [ ] Pr√™t pour d√©monstration

---

## üîß Commandes Utiles

### Astro CLI
```powershell
# D√©marrer Airflow
astro dev start

# Arr√™ter Airflow
astro dev stop

# Red√©marrer Airflow
astro dev restart

# Voir les logs
astro dev logs

# Bash dans le conteneur
astro dev bash

# Ex√©cuter les tests
astro dev pytest

# Valider les DAGs
astro dev parse
```

### Airflow CLI (dans le conteneur)
```bash
# Lister les DAGs
airflow dags list

# Tester un DAG
airflow dags test produce_JSON 2025-10-02

# Lister les t√¢ches
airflow tasks list produce_JSON

# Voir les variables
airflow variables list

# Voir les connexions
airflow connections list
```

### PostgreSQL
```bash
# Se connecter
psql -h postgres -U postgres -d youtube_dwh

# Commandes utiles
\dt staging.*     # Lister tables staging
\dt core.*        # Lister tables core
\d+ core.videos   # D√©crire une table
\q                # Quitter
```

---

## üìö Ressources

### Documentation
- [Astro CLI](https://docs.astronomer.io/astro/cli/overview)
- [Apache Airflow](https://airflow.apache.org/docs/)
- [YouTube API v3](https://developers.google.com/youtube/v3)
- [Soda Core](https://docs.soda.io/soda-core/)
- [PostgreSQL](https://www.postgresql.org/docs/)

### Tutoriels
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [ELT vs ETL](https://www.astronomer.io/blog/etl-vs-elt/)

---

## üéì Conseils

1. **Tester fr√©quemment**: Apr√®s chaque √©tape, tester le code
2. **Commiter souvent**: Faire des commits r√©guliers avec messages clairs
3. **Documenter en avan√ßant**: Ne pas attendre la fin pour documenter
4. **G√©rer les erreurs**: Pr√©voir les cas d'erreur d√®s le d√©but
5. **Respecter les quotas API**: Ne pas d√©passer 10000 unit√©s/jour
6. **Versionner les donn√©es**: Garder l'historique des extractions

---

## ‚è±Ô∏è Estimation du Temps par Phase

| Phase | Dur√©e | T√¢ches |
|-------|-------|--------|
| Setup | 2h | Astro init, configuration |
| DAG Extract | 4h | Cr√©ation, tests |
| PostgreSQL | 6h | Sch√©mas, DAG load, tests |
| Data Quality | 4h | Soda checks, DAG quality |
| Orchestration | 2h | DAG principal |
| Tests | 8h | 20+ tests unitaires/int√©gration |
| CI/CD | 4h | GitHub Actions |
| Dashboard | 4h | Streamlit (bonus) |
| Documentation | 4h | README, captures |
| **TOTAL** | **38h** | Sur 10 jours = 4h/jour |

---

## üìß Support

Si vous rencontrez des probl√®mes:
1. V√©rifier les logs Airflow
2. V√©rifier les connexions PostgreSQL
3. V√©rifier les quotas API YouTube
4. Consulter la documentation

---

**Bon courage pour votre projet! üöÄ**
