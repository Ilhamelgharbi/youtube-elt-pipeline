i want to do this project lets start with astro dev init and first dag , YouTube ELT Pipeline : Data Engineering avec Apache Airflow et PostgreSQL
Assigné
fr

OH
Omar Hitar
créé : 15/09/25
Conception d'un pipeline ELT automatisé pour l'analyse de données YouTube, développé avec Apache Airflow, PostgreSQL et validation de qualité des données.
Référentiels
Compétences transversales
[2023] Certification RNCP Développeur.se en intelligence artificielle
Ressources
Exemple de Format JSON Attendu
How to Get YouTube API Key (Step-by-Step Guide)
Quotas & Limits Link 1
Quotas & Limits Link 2
Airflow Tutorial
Contexte du projet
De nombreuses entreprises du secteur digital souhaitent automatiser l'analyse des performances de contenu YouTube pour optimiser leurs stratégies marketing et comprendre les tendances du marché.

Dans ce projet, vous allez construire un pipeline de data engineering complet qui combine :

Extraction automatisée des données YouTube via l'API v3,
Orchestration avec Apache Airflow et Astro CLI,
Data Warehouse PostgreSQL avec architecture staging/core,
Validation de qualité des données avec Soda Core,
Containerisation Docker pour la portabilité,
CI/CD automatisé avec GitHub Actions.
👩‍💻 User Stories

En tant que Data Engineer, je peux orchestrer l'extraction quotidienne des données YouTube via des DAGs Airflow.
En tant qu'Analyste de données, je peux accéder aux données structurées dans PostgreSQL via des outils locaux (pgAdmin, DBeaver) ou depuis Airflow.
En tant qu'DevOps, je peux déployer et monitorer le pipeline via Docker et Astro CLI.
En tant que Data Quality Manager, je peux valider automatiquement la qualité des données avec Soda Core.
En tant que Développeur, je peux lancer le pipeline localement et le déployer avec GitHub Actions.
(Bonus) En tant que Business Analyst, je peux visualiser les données via un dashboard simple.
Modalités pédagogiques
Travail : individuel
Durée : 10 jours ouvrés
Période : Du 15/09/2025 au 03/10/2025 avant minuit.
Outils : Python, Apache Airflow, Astro CLI, PostgreSQL, Docker, Soda Core, GitHub Actions
Bonus : Dashboard Streamlit, multi-chaînes YouTube, analytics avancées
Modalités d'évaluation
Démonstration (15 min) : Pipeline complet en local, exécution des DAGs, validation des données, accès PostgreSQL
Revue de code (10 min) : Structure des DAGs, architecture ELT, qualité du code, tests
Questions techniques (15 min) : Orchestration Airflow, Data Warehouse, validation qualité, déploiement Docker
Livrables
1. Un dépôt GitHub avec :
   • Code structuré (DAGs, modules, scripts, tests)
   • Configuration Astro CLI et Docker
   • Scripts de copie et d'automatisation
   • README complet avec instructions de déploiement
   • Pipeline CI/CD fonctionnel

2. Pipeline ELT opérationnel :
   • DAG produce_JSON : Extraction des données YouTube
   • DAG update_db : Chargement en PostgreSQL
   • DAG data_quality : Validation avec Soda Core

3. Captures d'écran : Interface Airflow, données PostgreSQL, logs d'exécution, tests de qualité

4. (Bonus) Dashboard de visualisation des données
Critères de performance
1. Pipeline ELT (Fonctionnalité)
• DAG produce_JSON :
      • Extraction des données YouTube :
      • Chaîne cible : MrBeast (@MrBeast) - Chaîne YouTube
      • Données extraites : ID vidéo, titre, date de publication, durée, nombre de vues, likes, commentaires
      • Format de sortie : Fichiers JSON timestampés (voir Ressources)
      • Gestion de la pagination et des quotas API (10000 unités/jour)
      • Sauvegarde en fichiers JSON avec horodatage
      • Gestion d'erreurs et retry logic
• DAG update_db :
      • Chargement des données en schéma staging
      • Transformation et nettoyage des données
      • Chargement en schéma core (Data Warehouse)
      • Gestion des doublons et de l'historique
• DAG data_quality :
      • Validation automatique avec Soda Core
      • Tests de complétude, cohérence et format
      • Alertes en cas de problème de qualité

2. Architecture & Code
      • Structure modulaire : DAGs séparés, modules réutilisables, configuration centralisée
      • Code lisible : Commentaires, docstrings, nommage clair
      • Gestion des secrets : Variables Airflow pour les clés API
      • Tests : Tests unitaires et d'intégration (minimum 20 tests)

3. Data Warehouse PostgreSQL
     • Architecture staging/core :
             • Schéma staging : Données brutes
             • Schéma core : Données transformées et nettoyées
     • Tables structurées : Colonnes typées, index, contraint
     • Transformations implémentées :
            • Conversion et optimisation des formats de données
            • Classification automatique des types de contenu
            • Nettoyage et standardisation des données
     • Historique : Conservation des données avec timestamps
     • Accès multi-canaux :
            • Depuis Airflow (hooks et requêtes)
            • Depuis outils locaux (pgAdmin, DBeaver, psql)

4. Déploiement & Infrastructure
      • Docker : Containerisation complète avec Astro CLI
      • Volumes : Synchronisation des données entre conteneur et host
      • Scripts d'automatisation : Copie des fichiers, tests, déploiement
      • CI/CD : Pipeline GitHub Actions fonctionnel

5. Validation & Qualité
      • Soda Core : Configuration des règles de qualité
      • Tests de données : Validation des métriques, formats, cohérence
      • Monitoring : Logs détaillés et alertes
      • Documentation : README complet et commentaires

6. Monitoring & Logs
     • Interface Airflow : Monitoring des DAGs et des tâches
     • Logs détaillés : Suivi des exécutions et des erreurs
     • Validation qualité : Rapports Soda Core automatiques
     • Tests de santé : Vérification de la connectivité et des données

⚠️ Points d'attention :
Respecter les quotas de l'API YouTube (10000 unités/jour)
Gérer les erreurs de connexion et les timeouts
Valider la qualité des données à chaque étape
Documenter chaque composant du pipeline
Tester l'ensemble du workflow avant la soumission // from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import json
from googleapiclient.discovery import build
import isodate

# 🔑 API config
API_KEY = "AIzaSyDyhE7sl0qtEhsZX0e8WXSdYNR6bEikHXQ"
CHANNEL_ID = "UCX6OQ3DkcsbYNE6H8uQQuVA"
CHANNEL_HANDLE = "MrBeast"

# YouTube client
youtube = build("youtube", "v3", developerKey=API_KEY)

def iso_duration_to_readable(duration):
    td = isodate.parse_duration(duration)
    total_seconds = int(td.total_seconds())
    minutes, seconds = divmod(total_seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours}:{minutes:02d}:{seconds:02d}" if hours > 0 else f"{minutes}:{seconds:02d}"

def get_channel_videos(**context):
    request = youtube.channels().list(part="contentDetails", id=CHANNEL_ID)
    response = request.execute()
    uploads_playlist = response["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

    request = youtube.playlistItems().list(
        part="snippet,contentDetails",
        playlistId=uploads_playlist,
        maxResults=2
    )
    response = request.execute()
    video_ids = [item["contentDetails"]["videoId"] for item in response["items"]]

    request = youtube.videos().list(
        part="snippet,contentDetails,statistics",
        id=",".join(video_ids)
    )
    videos_response = request.execute()

    videos = []
    for video in videos_response["items"]:
        duration = video["contentDetails"]["duration"]
        videos.append({
            "title": video["snippet"]["title"],
            "duration": duration,
            "video_id": video["id"],
            "like_count": video["statistics"].get("likeCount"),
            "view_count": video["statistics"].get("viewCount"),
            "published_at": video["snippet"]["publishedAt"],
            "comment_count": video["statistics"].get("commentCount"),
            "duration_readable": iso_duration_to_readable(duration)
        })

    output = {
        "channel_handle": CHANNEL_HANDLE,
        "extraction_date": datetime.now().isoformat(),
        "total_videos": len(videos),
        "videos": videos
    }

    os.makedirs("include/youtube_data", exist_ok=True)
    filename = f"include/youtube_data/{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=4)

    print(f"✅ Saved {len(videos)} videos → {filename}")

# DAG definition
default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="produce_JSON",
    default_args=default_args,
    description="Extract YouTube data and save as JSON",
    schedule="0 14 * * *",
    start_date=datetime(2025, 9, 1),
    catchup=False,
) as dag:

    extract_task = PythonOperator(
        task_id="extract_youtube_videos",
        python_callable=get_channel_videos,
        
    )


exemple de json 
{
    "channel_handle": "MrBeast",
    "extraction_date": "2025-09-14T23:28:15.195747",
    "total_videos": 2,
    "videos": [
        {
            "title": "Survive 30 Days Chained To Your Ex, Win $250,000",
            "duration": "PT37M4S",
            "video_id": "4l97aNza_Zc",
            "like_count": "1833636",
            "view_count": "54506132",
            "published_at": "2025-09-13T16:00:01Z",
            "comment_count": "27466",
            "duration_readable": "37:04"
        },
        {
            "title": "I Arrested IShowSpeed",
            "duration": "PT35S",
            "video_id": "3ih2bPKSWsQ",
            "like_count": "1058130",
            "view_count": "30850222",
            "published_at": "2025-09-12T17:00:01Z",
            "comment_count": "3265",
            "duration_readable": "0:35"
        }
    ]
