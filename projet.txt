i want to do this project lets start with astro dev init and first dag , YouTube ELT Pipeline : Data Engineering avec Apache Airflow et PostgreSQL
AssignÃ©
fr

OH
Omar Hitar
crÃ©Ã© : 15/09/25
Conception d'un pipeline ELT automatisÃ© pour l'analyse de donnÃ©es YouTube, dÃ©veloppÃ© avec Apache Airflow, PostgreSQL et validation de qualitÃ© des donnÃ©es.
ReÌfeÌrentiels
CompÃ©tences transversales
[2023] Certification RNCP DÃ©veloppeur.se en intelligence artificielle
Ressources
Exemple de Format JSON Attendu
How to Get YouTube API Key (Step-by-Step Guide)
Quotas & Limits Link 1
Quotas & Limits Link 2
Airflow Tutorial
Contexte du projet
De nombreuses entreprises du secteur digital souhaitent automatiser l'analyse des performances de contenu YouTube pour optimiser leurs stratÃ©gies marketing et comprendre les tendances du marchÃ©.

Dans ce projet, vous allez construire un pipeline de data engineering complet qui combine :

Extraction automatisÃ©e des donnÃ©es YouTube via l'API v3,
Orchestration avec Apache Airflow et Astro CLI,
Data Warehouse PostgreSQL avec architecture staging/core,
Validation de qualitÃ© des donnÃ©es avec Soda Core,
Containerisation Docker pour la portabilitÃ©,
CI/CD automatisÃ© avec GitHub Actions.
ðŸ‘©â€ðŸ’» User Stories

En tant que Data Engineer, je peux orchestrer l'extraction quotidienne des donnÃ©es YouTube via des DAGs Airflow.
En tant qu'Analyste de donnÃ©es, je peux accÃ©der aux donnÃ©es structurÃ©es dans PostgreSQL via des outils locaux (pgAdmin, DBeaver) ou depuis Airflow.
En tant qu'DevOps, je peux dÃ©ployer et monitorer le pipeline via Docker et Astro CLI.
En tant que Data Quality Manager, je peux valider automatiquement la qualitÃ© des donnÃ©es avec Soda Core.
En tant que DÃ©veloppeur, je peux lancer le pipeline localement et le dÃ©ployer avec GitHub Actions.
(Bonus) En tant que Business Analyst, je peux visualiser les donnÃ©es via un dashboard simple.
ModaliteÌs pÃ©dagogiques
Travail : individuel
DurÃ©e : 10 jours ouvrÃ©s
PÃ©riode : Du 15/09/2025 au 03/10/2025 avant minuit.
Outils : Python, Apache Airflow, Astro CLI, PostgreSQL, Docker, Soda Core, GitHub Actions
Bonus : Dashboard Streamlit, multi-chaÃ®nes YouTube, analytics avancÃ©es
ModalitÃ©s d'Ã©valuation
DÃ©monstration (15 min) : Pipeline complet en local, exÃ©cution des DAGs, validation des donnÃ©es, accÃ¨s PostgreSQL
Revue de code (10 min) : Structure des DAGs, architecture ELT, qualitÃ© du code, tests
Questions techniques (15 min) : Orchestration Airflow, Data Warehouse, validation qualitÃ©, dÃ©ploiement Docker
Livrables
1. Un dÃ©pÃ´t GitHub avec :
   â€¢ Code structurÃ© (DAGs, modules, scripts, tests)
   â€¢ Configuration Astro CLI et Docker
   â€¢ Scripts de copie et d'automatisation
   â€¢ README complet avec instructions de dÃ©ploiement
   â€¢ Pipeline CI/CD fonctionnel

2. Pipeline ELT opÃ©rationnel :
   â€¢ DAG produce_JSON : Extraction des donnÃ©es YouTube
   â€¢ DAG update_db : Chargement en PostgreSQL
   â€¢ DAG data_quality : Validation avec Soda Core

3. Captures d'Ã©cran : Interface Airflow, donnÃ©es PostgreSQL, logs d'exÃ©cution, tests de qualitÃ©

4. (Bonus) Dashboard de visualisation des donnÃ©es
CritÃ¨res de performance
1. Pipeline ELT (FonctionnalitÃ©)
â€¢ DAG produce_JSON :
      â€¢ Extraction des donnÃ©es YouTube :
      â€¢ ChaÃ®ne cible : MrBeast (@MrBeast) - ChaÃ®ne YouTube
      â€¢ DonnÃ©es extraites : ID vidÃ©o, titre, date de publication, durÃ©e, nombre de vues, likes, commentaires
      â€¢ Format de sortie : Fichiers JSON timestampÃ©s (voir Ressources)
      â€¢ Gestion de la pagination et des quotas API (10000 unitÃ©s/jour)
      â€¢ Sauvegarde en fichiers JSON avec horodatage
      â€¢ Gestion d'erreurs et retry logic
â€¢ DAG update_db :
      â€¢ Chargement des donnÃ©es en schÃ©ma staging
      â€¢ Transformation et nettoyage des donnÃ©es
      â€¢ Chargement en schÃ©ma core (Data Warehouse)
      â€¢ Gestion des doublons et de l'historique
â€¢ DAG data_quality :
      â€¢ Validation automatique avec Soda Core
      â€¢ Tests de complÃ©tude, cohÃ©rence et format
      â€¢ Alertes en cas de problÃ¨me de qualitÃ©

2. Architecture & Code
      â€¢ Structure modulaire : DAGs sÃ©parÃ©s, modules rÃ©utilisables, configuration centralisÃ©e
      â€¢ Code lisible : Commentaires, docstrings, nommage clair
      â€¢ Gestion des secrets : Variables Airflow pour les clÃ©s API
      â€¢ Tests : Tests unitaires et d'intÃ©gration (minimum 20 tests)

3. Data Warehouse PostgreSQL
     â€¢ Architecture staging/core :
             â€¢ SchÃ©ma staging : DonnÃ©es brutes
             â€¢ SchÃ©ma core : DonnÃ©es transformÃ©es et nettoyÃ©es
     â€¢ Tables structurÃ©es : Colonnes typÃ©es, index, contraint
     â€¢ Transformations implÃ©mentÃ©es :
            â€¢ Conversion et optimisation des formats de donnÃ©es
            â€¢ Classification automatique des types de contenu
            â€¢ Nettoyage et standardisation des donnÃ©es
     â€¢ Historique : Conservation des donnÃ©es avec timestamps
     â€¢ AccÃ¨s multi-canaux :
            â€¢ Depuis Airflow (hooks et requÃªtes)
            â€¢ Depuis outils locaux (pgAdmin, DBeaver, psql)

4. DÃ©ploiement & Infrastructure
      â€¢ Docker : Containerisation complÃ¨te avec Astro CLI
      â€¢ Volumes : Synchronisation des donnÃ©es entre conteneur et host
      â€¢ Scripts d'automatisation : Copie des fichiers, tests, dÃ©ploiement
      â€¢ CI/CD : Pipeline GitHub Actions fonctionnel

5. Validation & QualitÃ©
      â€¢ Soda Core : Configuration des rÃ¨gles de qualitÃ©
      â€¢ Tests de donnÃ©es : Validation des mÃ©triques, formats, cohÃ©rence
      â€¢ Monitoring : Logs dÃ©taillÃ©s et alertes
      â€¢ Documentation : README complet et commentaires

6. Monitoring & Logs
     â€¢ Interface Airflow : Monitoring des DAGs et des tÃ¢ches
     â€¢ Logs dÃ©taillÃ©s : Suivi des exÃ©cutions et des erreurs
     â€¢ Validation qualitÃ© : Rapports Soda Core automatiques
     â€¢ Tests de santÃ© : VÃ©rification de la connectivitÃ© et des donnÃ©es

âš ï¸ Points d'attention :
Respecter les quotas de l'API YouTube (10000 unitÃ©s/jour)
GÃ©rer les erreurs de connexion et les timeouts
Valider la qualitÃ© des donnÃ©es Ã  chaque Ã©tape
Documenter chaque composant du pipeline
Tester l'ensemble du workflow avant la soumission // from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import json
from googleapiclient.discovery import build
import isodate

# ðŸ”‘ API config
API_KEY = "AIzaSyDyhE7sl0qtEhsZX0e8WXSdYNR6bEikHXQ"
CHANNEL_ID = "UCX6OQ3DkcsbYNE6H8uQQuVA"
CHANNEL_HANDLE = "MrBeast"

# YouTube client
youtube = build("youtube", "v3", developerKey=API_KEY)

def iso_duration_to_readable(duration):
    td = isodate.parse_duration(duration)
    total_seconds = int(td.total_seconds())
    minutes, seconds = divmod(total_seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours}:{minutes:02d}:{seconds:02d}" if hours > 0 else f"{minutes}:{seconds:02d}"

def get_channel_videos(**context):
    request = youtube.channels().list(part="contentDetails", id=CHANNEL_ID)
    response = request.execute()
    uploads_playlist = response["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

    request = youtube.playlistItems().list(
        part="snippet,contentDetails",
        playlistId=uploads_playlist,
        maxResults=2
    )
    response = request.execute()
    video_ids = [item["contentDetails"]["videoId"] for item in response["items"]]

    request = youtube.videos().list(
        part="snippet,contentDetails,statistics",
        id=",".join(video_ids)
    )
    videos_response = request.execute()

    videos = []
    for video in videos_response["items"]:
        duration = video["contentDetails"]["duration"]
        videos.append({
            "title": video["snippet"]["title"],
            "duration": duration,
            "video_id": video["id"],
            "like_count": video["statistics"].get("likeCount"),
            "view_count": video["statistics"].get("viewCount"),
            "published_at": video["snippet"]["publishedAt"],
            "comment_count": video["statistics"].get("commentCount"),
            "duration_readable": iso_duration_to_readable(duration)
        })

    output = {
        "channel_handle": CHANNEL_HANDLE,
        "extraction_date": datetime.now().isoformat(),
        "total_videos": len(videos),
        "videos": videos
    }

    os.makedirs("include/youtube_data", exist_ok=True)
    filename = f"include/youtube_data/{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=4)

    print(f"âœ… Saved {len(videos)} videos â†’ {filename}")

# DAG definition
default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="produce_JSON",
    default_args=default_args,
    description="Extract YouTube data and save as JSON",
    schedule="0 14 * * *",
    start_date=datetime(2025, 9, 1),
    catchup=False,
) as dag:

    extract_task = PythonOperator(
        task_id="extract_youtube_videos",
        python_callable=get_channel_videos,
        
    )


exemple de json 
{
    "channel_handle": "MrBeast",
    "extraction_date": "2025-09-14T23:28:15.195747",
    "total_videos": 2,
    "videos": [
        {
            "title": "Survive 30 Days Chained To Your Ex, Win $250,000",
            "duration": "PT37M4S",
            "video_id": "4l97aNza_Zc",
            "like_count": "1833636",
            "view_count": "54506132",
            "published_at": "2025-09-13T16:00:01Z",
            "comment_count": "27466",
            "duration_readable": "37:04"
        },
        {
            "title": "I Arrested IShowSpeed",
            "duration": "PT35S",
            "video_id": "3ih2bPKSWsQ",
            "like_count": "1058130",
            "view_count": "30850222",
            "published_at": "2025-09-12T17:00:01Z",
            "comment_count": "3265",
            "duration_readable": "0:35"
        }
    ]
