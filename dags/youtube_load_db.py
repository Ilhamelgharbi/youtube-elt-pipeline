"""
üéØ YouTube ELT Pipeline - DAG: update_db
==========================================
DAG de chargement et transformation des donn√©es YouTube en PostgreSQL

Fonctionnalit√©s:
1Ô∏è‚É£ Synchronisation JSON ‚Üí Staging (UPSERT + DELETE)
2Ô∏è‚É£ Transformation Staging ‚Üí Core (avec analyse de dur√©e)
3Ô∏è‚É£ D√©clenchement des contr√¥les qualit√©

Architecture:
- Sch√©ma staging: Donn√©es brutes YouTube
- Sch√©ma core: Donn√©es transform√©es et enrichies
- Gestion automatique des doublons et de l'historique

Orchestration:
produce_JSON ‚Üí update_db ‚Üí data_quality
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path

# ==========================================
# üîß CONFIGURATION
# ==========================================
DATA_PATH = "/usr/local/airflow/include/youtube_data"
POSTGRES_CONN_ID = "postgres_dwh"


def find_latest_json(**context):
    """
    üîç Recherche du fichier JSON le plus r√©cent
    
    Returns:
        str: Chemin absolu du fichier JSON le plus r√©cent
    
    Raises:
        FileNotFoundError: Si aucun fichier JSON n'est trouv√©
    """
    json_files = list(Path(DATA_PATH).glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"‚ùå Aucun fichier JSON trouv√© dans {DATA_PATH}")
    
    latest = max(json_files, key=lambda f: f.stat().st_mtime)
    logging.info(f"‚úÖ Fichier JSON s√©lectionn√©: {latest.name}")
    
    context['ti'].xcom_push(key='json_file', value=str(latest))
    return str(latest)


def sync_to_staging(**context):
    """
    üì• Synchronisation JSON ‚Üí Sch√©ma Staging
    
    Op√©rations:
    - UPSERT: Insertion de nouvelles vid√©os, mise √† jour des existantes
    - DELETE: Suppression des vid√©os absentes du JSON
    - D√©doublonnage: Garantie d'unicit√© des video_ids
    
    Returns:
        int: Nombre de vid√©os synchronis√©es
    """
    # R√©cup√©ration du fichier JSON
    json_file = context['ti'].xcom_pull(task_ids='find_latest_json', key='json_file')
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    videos = data['videos']
    channel_id = data['channel_id']
    channel_handle = data['channel_handle']
    extraction_date = data['extraction_date']
    
    # D√©doublonnage dans le JSON
    unique_videos = {v['video_id']: v for v in videos}
    videos = list(unique_videos.values())
    
    logging.info(f"üìä {len(videos)} vid√©os uniques extraites du JSON")
    
    # Connexion √† PostgreSQL
    pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    
    # R√©cup√©ration des IDs existants en staging
    existing_ids = set()
    try:
        result = pg_hook.get_records("SELECT video_id FROM staging.youtube_videos_raw")
        existing_ids = {row[0] for row in result}
        logging.info(f"üìÇ {len(existing_ids)} vid√©os d√©j√† en staging")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les IDs existants: {e}")
    
    json_video_ids = {v['video_id'] for v in videos}
    
    # ==========================================
    # UPSERT: Insertion et mise √† jour
    # ==========================================
    upsert_query = """
        INSERT INTO staging.youtube_videos_raw (
            video_id, title, published_at, duration, duration_readable,
            view_count, like_count, comment_count,
            channel_id, channel_handle, extraction_date
        ) VALUES (
            %(video_id)s, %(title)s, %(published_at)s, %(duration)s, %(duration_readable)s,
            %(view_count)s, %(like_count)s, %(comment_count)s,
            %(channel_id)s, %(channel_handle)s, %(extraction_date)s
        )
        ON CONFLICT (video_id) DO UPDATE SET
            title = EXCLUDED.title,
            view_count = EXCLUDED.view_count,
            like_count = EXCLUDED.like_count,
            comment_count = EXCLUDED.comment_count,
            extraction_date = EXCLUDED.extraction_date
    """
    
    inserted = updated = 0
    for video in videos:
        # Gestion des valeurs None pour les statistiques
        view_count = video.get('view_count')
        like_count = video.get('like_count')
        comment_count = video.get('comment_count')
        
        pg_hook.run(upsert_query, parameters={
            'video_id': video['video_id'],
            'title': video['title'],
            'published_at': video['published_at'],
            'duration': video['duration'],
            'duration_readable': video['duration_readable'],
            'view_count': int(view_count) if view_count is not None else 0,
            'like_count': int(like_count) if like_count is not None else 0,
            'comment_count': int(comment_count) if comment_count is not None else 0,
            'channel_id': channel_id,
            'channel_handle': channel_handle,
            'extraction_date': extraction_date,
        })
        if video['video_id'] in existing_ids:
            updated += 1
        else:
            inserted += 1
    
    logging.info(f"‚úÖ Insertions: {inserted} | Mises √† jour: {updated}")
    
    # ==========================================
    # DELETE: Suppression des vid√©os obsol√®tes
    # ==========================================
    to_delete = existing_ids - json_video_ids
    if to_delete:
        pg_hook.run(
            "DELETE FROM staging.youtube_videos_raw WHERE video_id = ANY(%(ids)s)",
            parameters={'ids': list(to_delete)}
        )
        logging.info(f"üóëÔ∏è Supprim√©es: {len(to_delete)} vid√©os obsol√®tes")
    
    # Passer les donn√©es √† la t√¢che suivante
    context['ti'].xcom_push(key='extraction_date', value=extraction_date)
    return len(videos)


def transform_to_core(**context):
    """
    üîÑ Transformation Staging ‚Üí Core Tables
    
    Transformations appliqu√©es:
    - Conversion ISO 8601 (PT22M26S) ‚Üí PostgreSQL INTERVAL
    - Labellisation: 'short' (<1 min) ou 'long' (‚â•1 min)
    - Gestion de l'historique: created_at, updated_at
    
    Tables cibles:
    - core.videos: M√©tadonn√©es enrichies des vid√©os
    - core.video_statistics: Historique des statistiques
    
    Returns:
        int: Nombre de vid√©os transform√©es
    """
    extraction_date = context['ti'].xcom_pull(task_ids='sync_to_staging', key='extraction_date')
    pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    
    logging.info("üîÑ D√©but de la transformation staging ‚Üí core...")
    
    # ==========================================
    # UPSERT dans core.videos avec transformations
    # ==========================================
    pg_hook.run("""
        WITH duration_calc AS (
            SELECT 
                video_id,
                title,
                published_at,
                duration,
                duration_readable,
                channel_id,
                channel_handle,
                -- Conversion ISO 8601 (PT22M26S) ‚Üí INTERVAL PostgreSQL
                duration::INTERVAL AS duration_seconds,
                -- Calcul des secondes totales pour labellisation
                EXTRACT(EPOCH FROM duration::interval) AS total_seconds
            FROM staging.youtube_videos_raw
        )
        INSERT INTO core.videos (
            video_id, title, published_at, duration, duration_readable,
            duration_seconds, duration_label,
            channel_id, channel_handle, created_at, updated_at
        )
        SELECT DISTINCT 
            video_id,
            title,
            published_at,
            duration,
            duration_readable,
            duration_seconds,
            CASE 
                WHEN total_seconds < 60 THEN 'short'
                ELSE 'long'
            END AS duration_label,
            channel_id,
            channel_handle,
            NOW(),
            NOW()
        FROM duration_calc
        ON CONFLICT (video_id) DO UPDATE SET
            title = EXCLUDED.title,
            duration_seconds = EXCLUDED.duration_seconds,
            duration_label = EXCLUDED.duration_label,
            updated_at = NOW()
    """)
    
    logging.info("‚úÖ Table core.videos mise √† jour")
    
    # ==========================================
    # DELETE: Suppression des vid√©os absentes de staging
    # ==========================================
    # First, count how many will be deleted
    to_delete_core = pg_hook.get_first("""
        SELECT COUNT(*) FROM core.videos
        WHERE video_id NOT IN (
            SELECT video_id FROM staging.youtube_videos_raw
        )
    """)[0]
    
    # Then delete them
    if to_delete_core > 0:
        pg_hook.run("""
            DELETE FROM core.videos
            WHERE video_id NOT IN (
                SELECT video_id FROM staging.youtube_videos_raw
            )
        """)
        logging.info(f"üóëÔ∏è Supprim√©es de core.videos: {to_delete_core} vid√©os absentes de staging")
    else:
        logging.info("‚úÖ Aucune vid√©o √† supprimer de core.videos")
    
    # ==========================================
    # INSERT dans core.video_statistics (historique)
    # ==========================================
    pg_hook.run("""
        INSERT INTO core.video_statistics (
            video_id, view_count, like_count, comment_count, recorded_at
        )
        SELECT video_id, view_count, like_count, comment_count, %(date)s::timestamp
        FROM staging.youtube_videos_raw
    """, parameters={'date': extraction_date})
    
    logging.info("‚úÖ Table core.video_statistics mise √† jour")
    
    # Comptage final
    count = pg_hook.get_first("SELECT COUNT(*) FROM core.videos")[0]
    logging.info(f"‚úÖ Transformation termin√©e: {count} vid√©os dans core.videos")
    return count


# ==========================================
# üéØ CONFIGURATION DU DAG
# ==========================================

default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 9, 1),
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="update_db",
    description="üì• Load YouTube JSON ‚Üí PostgreSQL (staging ‚Üí core avec transformations)",
    default_args=default_args,
    schedule=None,  # D√©clench√© automatiquement par produce_JSON
    catchup=False,
    tags=["youtube", "etl", "postgresql"],
) as dag:
    
    # ==========================================
    # üìã D√âFINITION DES T√ÇCHES
    # ==========================================
    
    find_json = PythonOperator(
        task_id="find_latest_json",
        python_callable=find_latest_json,
        doc_md="""
        ### Recherche du fichier JSON
        Identifie le fichier JSON le plus r√©cent dans `/include/youtube_data/`
        """,
    )
    
    sync_staging = PythonOperator(
        task_id="sync_to_staging",
        python_callable=sync_to_staging,
        doc_md="""
        ### Synchronisation vers Staging
        - UPSERT: Insertion/mise √† jour des vid√©os
        - DELETE: Suppression des vid√©os obsol√®tes
        - Garantie d'unicit√© des video_ids
        """,
    )
    
    transform = PythonOperator(
        task_id="transform_to_core",
        python_callable=transform_to_core,
        doc_md="""
        ### Transformation vers Core
        - Conversion ISO 8601 ‚Üí PostgreSQL INTERVAL
        - Labellisation: 'short' (<1 min) / 'long' (‚â•1 min)
        - Historique des statistiques
        """,
    )
    
    trigger_quality = TriggerDagRunOperator(
        task_id="trigger_data_quality",
        trigger_dag_id="data_quality",
        wait_for_completion=False,
        doc_md="""
        ### D√©clenchement des contr√¥les qualit√©
        Lance automatiquement le DAG `data_quality` avec Soda Core
        """,
    )
    
    # ==========================================
    # üîó WORKFLOW
    # ==========================================
    find_json >> sync_staging >> transform >> trigger_quality
